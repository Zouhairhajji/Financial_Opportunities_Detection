{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def create_spark_context(master_ip='127.0.0.1'):\n",
    "    master_ip = 'spark://{}:7077'.format(master_ip)\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(master_ip)  \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    return (spark, sc)\n",
    "\n",
    "spark, sc = create_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- company_id: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- sector_name: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- NBI: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_filename = '../ressources/data/financial_sells_100000.csv'\n",
    "separator = ','\n",
    "\n",
    "init_flat_data = spark.read         \\\n",
    "    .option(\"sep\", separator)  \\\n",
    "    .csv(input_filename, header=True)\n",
    "\n",
    "init_flat_data = init_flat_data.fillna(0)\n",
    "init_flat_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+----------+--------------------+--------------------+----------+--------------------+\n",
      "|         sector_name|       country|        company_name|company_id|      product_bought|   all_product_group|diff_count|   unbought_products|\n",
      "+--------------------+--------------+--------------------+----------+--------------------+--------------------+----------+--------------------+\n",
      "|Metallurgical ind...|        France|  UnitedHealth Group|        49|Commodities,Asset...|Commodities,Expor...|         1|      Export Finance|\n",
      "| Electrical industry|         Italy|              Nestle|        11|Life and Dammage ...|Life and Dammage ...|         1|    Leverage Finance|\n",
      "|     Energy industry|        Suisse|Ping An Insurance...|        48|ALD Car Renting a...|ALD Car Renting a...|         1|           Financing|\n",
      "|     Energy industry|        Suisse|          ExxonMobil|        13|Financing,General...|ALD Car Renting a...|         1|ALD Car Renting a...|\n",
      "|   Chemical Industry|United Kinkdom|       HSBC Holdings|         2|Financing,Loans s...|Financing,Loans s...|         1|International Ret...|\n",
      "|   Chemical Industry|        France|       Roche Holding|         7|Financing,Loans s...|Financing,Loans s...|         1|General Purpose F...|\n",
      "|   Chemical Industry|        France|             Comcast|        42|Financing,General...|Financing,Loans s...|         1|   Loans syndication|\n",
      "| Electrical industry|         Italy|            Alphabet|        46|Others,Leverage F...|Life and Dammage ...|         1|Life and Dammage ...|\n",
      "|   Chemical Industry|United Kinkdom|  Berkshire Hathaway|        37|Financing,Loans s...|Financing,Loans s...|         1|     Issuer Services|\n",
      "|   Chemical Industry|         Corse|          Mastercard|        12|Loans syndication...|Financing,Loans s...|         1|           Financing|\n",
      "+--------------------+--------------+--------------------+----------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rdd_product_by_group = init_flat_data  \\\n",
    "        .groupby(['sector_name', 'country'])   \\\n",
    "        .agg( collect_set('product_name').alias('all_product_group') )    \n",
    "\n",
    "grouped_customers = init_flat_data  \\\n",
    "        .groupby(['sector_name', 'country', 'company_name', 'company_id'])    \\\n",
    "        .agg(collect_set('product_name').alias('product_bought')) \n",
    "\n",
    "\n",
    "# define two methods that calculate unbought products\n",
    "calculate_diff = udf(lambda x, y: len(set(x) - set(y)) , IntegerType())\n",
    "calculate_unbought_products = udf(lambda x, y: list(set(x) - set(y)) , ArrayType(StringType()))\n",
    "\n",
    "#convert array to string\n",
    "convert_array2str = udf(lambda x: ','.join(x) , StringType())\n",
    "\n",
    "detection_result = grouped_customers    \\\n",
    "    .join(rdd_product_by_group, on=['sector_name', 'country'])  \\\n",
    "    .withColumn('diff_count', calculate_diff('all_product_group', 'product_bought')) \\\n",
    "    .withColumn('unbought_products', calculate_unbought_products('all_product_group', 'product_bought')) \\\n",
    "    .filter('diff_count > 0')\n",
    "\n",
    "\n",
    "# remove array columns\n",
    "detection_result = detection_result \\\n",
    "    .withColumn('unbought_products', convert_array2str('unbought_products'))  \\\n",
    "    .withColumn('product_bought', convert_array2str('product_bought'))  \\\n",
    "    .withColumn('all_product_group', convert_array2str('all_product_group')) \n",
    "\n",
    "\n",
    "\n",
    "print( detection_result.show(10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_result.coalesce(1) \\\n",
    "    .write   \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save('../ressources/data/2_cross_selling_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 54835)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pyspark/accumulators.py\", line 269, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pyspark/serializers.py\", line 717, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# save result in my postgresql database\n",
    "mode = \"overwrite\"\n",
    "table_name = 'algo_cross_selling_analysis'\n",
    "url = \"jdbc:postgresql://127.0.0.1:5432/financial_opportunities\"\n",
    "properties = {\n",
    "    \"user\": \"zouhairhajji\",\n",
    "    \"password\": '',\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "detection_result  \\\n",
    "        .write     \\\n",
    "        .jdbc(url=url, table=table_name, mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
